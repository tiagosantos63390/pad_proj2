{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bce945e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import spacy\n",
    "import string\n",
    "import unicodedata\n",
    "import random\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d5b5845",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'corpus2mw'\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a7b9e9",
   "metadata": {},
   "source": [
    "# Part I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e71e29ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "\n",
    "    text = text.replace(\"’\", \"'\").replace(\"‘\", \"'\").replace(\"−\", \"-\").replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "\n",
    "    text = ''.join(c for c in text if not unicodedata.combining(c))\n",
    "\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r\"(?<=\\w)-(?=\\w)\", \"_\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s_']\", ' ', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de11d491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.is_punct or token.is_space:\n",
    "            continue\n",
    "\n",
    "        txt = token.text.lower()\n",
    "\n",
    "        if \"_\" in txt:\n",
    "            if re.match(r'^[a-z0-9_]+$', txt):\n",
    "                tokens.append(txt)\n",
    "                \n",
    "        else:\n",
    "            lemma = token.lemma_.lower()\n",
    "            if token.pos_ == \"VERB\" or token.like_num or token.is_alpha:\n",
    "                if re.match(r'^[a-z0-9]+$', lemma): \n",
    "                    tokens.append(lemma)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "930a94d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams(tokens, max_n):\n",
    "    result = Counter()\n",
    "\n",
    "    for n in range(1, max_n + 1):\n",
    "        ngrams = zip(*(islice(tokens, i, None) for i in range(n)))\n",
    "        result.update(ngrams)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff1d83d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_frequency(ngrams, min_freq):\n",
    "    return {k: v for k, v in ngrams.items() if v >= min_freq and len(k) > 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a08fe352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scp_score(ngram, ngram_counts, unigram_counts, alpha=0.01):\n",
    "    total_words = sum(unigram_counts.values())\n",
    "    vocab_size = len(unigram_counts)\n",
    "    joint_prob = (ngram_counts.get(ngram, 0) + alpha) / (total_words + alpha * vocab_size)\n",
    "    product = 1\n",
    "\n",
    "    for word in ngram:\n",
    "        p_word = (unigram_counts.get((word,), 0) + alpha) / (total_words + alpha * vocab_size)\n",
    "        product *= p_word\n",
    "\n",
    "    return (joint_prob ** 2) / product if product != 0 else 0\n",
    "\n",
    "\n",
    "def dice_score(ngram, ngram_counts, unigram_counts):\n",
    "    f_ngram = ngram_counts[ngram]\n",
    "    f_sum = sum(unigram_counts.get((w,), 0) for w in ngram)\n",
    "\n",
    "    return (len(ngram) * f_ngram) / f_sum if f_sum else 0\n",
    "\n",
    "\n",
    "def phi2_score(ngram, ngram_counts, unigram_counts):\n",
    "    O = ngram_counts[ngram]\n",
    "    N = sum(unigram_counts.values())\n",
    "    E = 1\n",
    "\n",
    "    for w in ngram:\n",
    "        E *= unigram_counts.get((w,), 1) / N\n",
    "\n",
    "    E *= N\n",
    "\n",
    "    return ((O - E) ** 2) / E if E else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f82f399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_local_max(ngram, scores, ngram_counts):\n",
    "    n = len(ngram)\n",
    "    score = scores.get(ngram, 0)\n",
    "\n",
    "    for i in range(n):\n",
    "        sub_ngram = ngram[:i] + ngram[i+1:]\n",
    "        if sub_ngram in scores and scores[sub_ngram] > score:\n",
    "            return False\n",
    "\n",
    "    for other_ngram in scores.keys():\n",
    "        if len(other_ngram) == n + 1:\n",
    "\n",
    "            for j in range(len(other_ngram) - n + 1):\n",
    "                if other_ngram[j:j+n] == ngram and scores[other_ngram] > score:\n",
    "                    return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def extract_local_max(scores, ngram_counts, min_len=2):\n",
    "    local_max_ngrams = {}\n",
    "\n",
    "    for ngram in scores:\n",
    "        if len(ngram) >= min_len and is_local_max(ngram, scores, ngram_counts):\n",
    "            local_max_ngrams[ngram] = scores[ngram]\n",
    "\n",
    "    return local_max_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f7b5a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_candidates(directory, max_files=None):\n",
    "    all_candidates = set()\n",
    "    \n",
    "    files = sorted([f for f in os.listdir(directory) if f.startswith('fil_')],\n",
    "                  key=lambda x: int(x.split('_')[1]))\n",
    "    \n",
    "    if max_files:\n",
    "        files = files[:max_files]\n",
    "    \n",
    "    for filename in tqdm(files, desc=\"Processing Files\"):\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as f:\n",
    "            original = f.read()\n",
    "        \n",
    "        preprocessed = preprocess_text(original)\n",
    "        tokens = tokenize(preprocessed)\n",
    "        unigrams = extract_ngrams(tokens, 1)\n",
    "        ngrams = extract_ngrams(tokens, 7)\n",
    "        filtered = filter_by_frequency(ngrams, 2)\n",
    "        \n",
    "        scp_scores = {ng: scp_score(ng, filtered, unigrams) for ng in filtered}\n",
    "        dice_scores = {ng: dice_score(ng, filtered, unigrams) for ng in filtered}\n",
    "        phi2_scores = {ng: phi2_score(ng, filtered, unigrams) for ng in filtered}\n",
    "        \n",
    "        sorted_scp_scores = dict(sorted(scp_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "        sorted_dice_scores = dict(sorted(dice_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "        sorted_phi2_scores = dict(sorted(phi2_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "        localmax_scp = extract_local_max(sorted_scp_scores, filtered)\n",
    "        localmax_dice = extract_local_max(sorted_dice_scores, filtered)\n",
    "        localmax_phi2 = extract_local_max(sorted_phi2_scores, filtered)\n",
    "        \n",
    "        for method in [localmax_scp, localmax_dice, localmax_phi2]:\n",
    "            for ngram in method:\n",
    "                all_candidates.add(' '.join(ngram))\n",
    "    \n",
    "    return sorted(all_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b910e0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 100%|██████████| 3170/3170 [08:49<00:00,  5.99it/s]\n"
     ]
    }
   ],
   "source": [
    "candidates = collect_all_candidates(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06450139",
   "metadata": {},
   "source": [
    "## Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b28c4f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_evaluation(candidates, sample_size, random_seed=None):\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "        \n",
    "    return random.sample(candidates, min(sample_size, len(candidates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bebf45fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_sample = precision_evaluation(candidates, 50)\n",
    "\n",
    "with open('precision_sample.txt', 'w', encoding='utf-8') as f:\n",
    "    for i, mwe in enumerate(precision_sample, 1):\n",
    "        f.write(f\"{mwe}\\n\")\n",
    "\n",
    "# after the creation of the precision sample file we need to edit it manually\n",
    "# writting TP (valid) or FP (not valid) for each relevant expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fac215fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(file):\n",
    "    valid = 0\n",
    "    total = 0\n",
    "    \n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip().endswith('TP'):\n",
    "                valid += 1\n",
    "            total += 1\n",
    "    \n",
    "    return valid / total if total > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0d2fe6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_precision('precision_sample.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493446ec",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2841aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paragraphs(directory, num_paragraphs, max_files=None):\n",
    "    files = sorted([f for f in os.listdir(directory) if f.startswith('fil_')],\n",
    "                  key=lambda x: int(x.split('_')[1]))\n",
    "    \n",
    "    if max_files:\n",
    "        files = files[:max_files]\n",
    "    \n",
    "    all_paragraphs = []\n",
    "    \n",
    "    for filename in files:\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "        all_paragraphs.extend(paragraphs)\n",
    "    \n",
    "    return random.sample(all_paragraphs, min(num_paragraphs, len(all_paragraphs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "116ae739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph 1:\n",
      "Asmadere, Beşiri\n",
      "==================================================\n",
      "\n",
      "Paragraph 2:\n",
      "Frederick commanded 4,500 infantry and 5 pieces of artillery to attack the east side of the city. When the Dutch forces reached the Eastern side of the city, they met a small Spanish force of 1,500 men. The Dutch started with several cannon shots which killed already 100 men. The Spanish where quickly approached by the Dutch forces. In 10 days, the Spanish lost 1,000 men and the Dutch lost 400 men. The Dutch now controlled the east side of the city.\n",
      "Itamar Assumpção was one of the biggest names and contributors in the \"Vanguarda Paulistana\" (\"São Paulo Vanguard\" in English), an alternative scene that dominated São Paulo in the very late 70s and the first half of the 80s. This movement united artists who operated involuntarily outside of the established commercial music industry during an era that pre-dated the Internet, the politics of downloading and the contemporary indie music scene. The \"Vanguarda\" was responsible for launching many new talents and establishing a foundation for autonomous, self-sufficient music production (and subsequent release) by artists such as Arrigo Barnabé and Grupo Rumo.\n",
      "Tan Kim Ching together with an English merchant in Singapore (W. H. M. Read) drafted a letter to Governor Sir Andrew Clarke, which Abdullah signed, in which Raja Muda Abdullah expressed his desire to place Perak under British protection, and \"to have a man of sufficient abilities to show him a good system of government.\"\n",
      "10 Hapoel Tayibe were also promoted as runners-up.\n",
      "==================================================\n",
      "\n",
      "Paragraph 3:\n",
      "St. Martin Secondary School\n",
      "Ferrin's most recent film is \"Easter Bunny, Kill! Kill!\". The DVD for \"Someone's Knocking at the Door\" was released on 25 May 2010. Ferrin's both last film \"Someone's Knocking at the Door\" and \"Easter Bunny Kill! Kill!\" are part of the Dark Delicacies 2010. He will direct the thriller film Dances With Werewolves in 3-D. He narrated his film \"Someone's Knocking at the Door\" on the re-releasing on 24 July 2010 in the Sci-Fi Center in Sin City.\n",
      "There was some friction with the governors, and Jeffreys resigned in 1931, taking up a post at Ottershaw College in Surrey.\n",
      "==================================================\n",
      "\n",
      "Paragraph 4:\n",
      "Yusu Station (Fukuoka)\n",
      "<doc id=\"24256373\" url=\"http://en.wikipedia.org/wiki?curid=24256373\" title=\"Braxton Kelley\">\n",
      "In August 2009, they decided on a more professional approach for their first official EP. It took two days to record \"I, Boheme\" in the Dutch Sahara Sound Studio in The Hague, where they worked with Henk Koorn (Hallo Venray). A music video for track \"Insomnia\" was shot soon after and both the video and the EP were released on November 5 in Supermarkt, The Hague. The EP was spotted by the Dutch pop magazine \"LiveXS\" and subsequently The Bohemes were chosen as Local Heroes of the January/February issue.\n",
      "Certainly a row at the time, but one that Evans later realised played a part in leading to his second marriage, this connected him with another bizarrerie. In all innocence he erected his usual Peter Evans Eating Houses sign over a new restaurant at the junction of Kensington High Street and Kensington Church Street [https://secure.flickr.com/photos/rollthedice/3390273363/]. The Eating House was bang next door to a church and the David Hicks's stylised PEEH fork was pointing directly at it. The Church elders were not amused. 'Definitely not Good Evans – His Devil's Fork Threatens The Church' summed up newspaper headlines around the world.\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paragraphs_sample = extract_paragraphs(directory, 4)\n",
    "\n",
    "for i, para in enumerate(paragraphs_sample, 1):\n",
    "    print(f\"Paragraph {i}:\\n{para}\\n{'='*50}\\n\")\n",
    "\n",
    "# after getting the sample paragraphs, we need to create the file recall_sample.txt\n",
    "# writting the relevant expressions found in each paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f62df85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(file, system_candidates):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        manual_mwes = set(line.strip() for line in f if line.strip())\n",
    "    \n",
    "    system_mwes = set(' '.join(ngram) for ngram in system_candidates)\n",
    "    \n",
    "    matches = 0\n",
    "    for mwe in manual_mwes:\n",
    "        if mwe.lower() in system_mwes:\n",
    "            matches += 1\n",
    "    \n",
    "    return matches / len(manual_mwes) if manual_mwes else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fc55ddf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_recall('recall_sample.txt', candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaa3649",
   "metadata": {},
   "source": [
    "# Part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c0ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_most_informative(localmax_scores, top_n=15):\n",
    "    sorted_res = sorted(\n",
    "        localmax_scores.items(),\n",
    "        key=lambda x: (x[1], len(x[0])),\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    return sorted_res[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c92c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity_matrix(ngram_sets):\n",
    "    all_res = list(set(' '.join(ng) for ngrams in ngram_sets for ng in ngrams))\n",
    "    \n",
    "    doc_texts = [' '.join(' '.join(ng) for ng in ngrams) for ngrams in ngram_sets]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(tokenizer=lambda x: x.split())\n",
    "    tfidf_matrix = vectorizer.fit_transform(doc_texts + all_res)\n",
    "\n",
    "    re_matrix = tfidf_matrix[-len(all_res):]\n",
    "    similarity = cosine_similarity(re_matrix)\n",
    "\n",
    "    return {\n",
    "        all_res[i]: {\n",
    "            all_res[j]: similarity[i][j] for j in range(len(all_res)) if i != j\n",
    "        } for i in range(len(all_res))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cca7887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_implicit_keywords(explicit_keywords, similarity_matrix, threshold=0.3, top_n=5):\n",
    "    implicit_keywords = set()\n",
    "    \n",
    "    for ek in explicit_keywords:\n",
    "        ek = ek.lower()\n",
    "        if ek not in similarity_matrix:\n",
    "            continue\n",
    "        \n",
    "        similar = similarity_matrix.get(ek, {})\n",
    "        candidates = [(re, sim) for re, sim in similar.items() if sim >= threshold and re != ek]\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for re, _ in candidates[:top_n]:\n",
    "            implicit_keywords.add(re)\n",
    "    \n",
    "    return list(implicit_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fb9ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents_for_keywords(directory, num_files=None):\n",
    "    files = sorted([f for f in os.listdir(directory) if f.startswith('fil_')],\n",
    "                  key=lambda x: int(x.split('_')[1]))\n",
    "    \n",
    "    if num_files:\n",
    "        files = files[:num_files]\n",
    "    \n",
    "    all_documents = []\n",
    "    all_explicit_keywords = []\n",
    "    all_ngrams = []\n",
    "    \n",
    "    for filename in tqdm(files, desc=\"Extracting REs\"):\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as f:\n",
    "            original = f.read()\n",
    "        \n",
    "        preprocessed = preprocess_text(original)\n",
    "        tokens = tokenize(preprocessed)\n",
    "        unigrams = extract_ngrams(tokens, 1)\n",
    "        ngrams = extract_ngrams(tokens, 7)\n",
    "        filtered = filter_by_frequency(ngrams, 2)\n",
    "        \n",
    "        scp_scores = {ng: scp_score(ng, filtered, unigrams) for ng in filtered}\n",
    "        dice_scores = {ng: dice_score(ng, filtered, unigrams) for ng in filtered}\n",
    "        phi2_scores = {ng: phi2_score(ng, filtered, unigrams) for ng in filtered}\n",
    "\n",
    "        sorted_scp_scores = dict(sorted(scp_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "        sorted_dice_scores = dict(sorted(dice_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "        sorted_phi2_scores = dict(sorted(phi2_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "        \n",
    "        localmax_scp = extract_local_max(sorted_scp_scores, filtered)\n",
    "        localmax_dice = extract_local_max(sorted_dice_scores, filtered)\n",
    "        localmax_phi2 = extract_local_max(sorted_phi2_scores, filtered)\n",
    "        \n",
    "        combined_localmax = {}\n",
    "        for method in [localmax_scp, localmax_dice, localmax_phi2]:\n",
    "            for ng, score in method.items():\n",
    "                key = ' '.join(ng)\n",
    "                if key not in combined_localmax or score > combined_localmax[key]:\n",
    "                    combined_localmax[key] = score\n",
    "        \n",
    "        informative_res = select_most_informative(combined_localmax)\n",
    "        \n",
    "        all_documents.append({\n",
    "            'filename': filename,\n",
    "            'original': original,\n",
    "            'preprocessed': preprocessed,\n",
    "            'tokens': tokens,\n",
    "            'localmax': combined_localmax,\n",
    "            'informative_res': informative_res\n",
    "        })\n",
    "        \n",
    "        all_ngrams.append(set(tuple(kw.split()) for kw in combined_localmax.keys()))\n",
    "        all_explicit_keywords.append([(kw, score) for kw, score in informative_res])\n",
    "    \n",
    "    similarity_matrix = calculate_similarity_matrix(all_ngrams)\n",
    "    \n",
    "    for i, doc in enumerate(all_documents):\n",
    "        explicit_keywords = [ek[0] for ek in all_explicit_keywords[i]]\n",
    "        implicit_keywords = find_implicit_keywords(\n",
    "            explicit_keywords, \n",
    "            similarity_matrix\n",
    "        )\n",
    "        \n",
    "        doc['explicit_keywords'] = explicit_keywords\n",
    "        doc['implicit_keywords'] = implicit_keywords\n",
    "    \n",
    "    return all_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1b24ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_with_keywords = process_documents_for_keywords(directory, num_files=1)\n",
    "    \n",
    "for doc in documents_with_keywords:\n",
    "    print(f\"\\nDocument: {doc['filename']}\")\n",
    "\n",
    "    print(\"\\nAll Local Maxs (REs):\")\n",
    "    for i, (kw, score) in enumerate(sorted(doc['localmax'].items(), key=lambda x: -x[1]), 1):\n",
    "        print(f\"{i}. {kw} (score: {score:.4f})\")\n",
    "\n",
    "    print(\"\\nExplicit Keywords (Top Informative REs):\")\n",
    "    for i, (kw, score) in enumerate(doc['informative_res'], 1):\n",
    "        print(f\"{i}. {kw} (score: {score:.4f})\")\n",
    "    \n",
    "    print(\"\\nImplicit Keywords (Similarity-Based):\")\n",
    "    for i, kw in enumerate(doc['implicit_keywords'], 1):\n",
    "        print(f\"{i}. {kw}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c625cdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfdi\n",
    "# implicitas: Score(T, di) = (SUMATÓRIO corr(T, Ki)) / i (Ki ∈ KeyWordsExplicitas(di))\n",
    "# coV(T, Ki) = (1 / len(DOCS)) * SUMATÓRIO ((P(T,d)-P(T, Docs)) *(P(Ki, d) - P(Ki, Docs))) / i (d ∈ Docs)\n",
    "# corr(T, Ki) = coV(T, Ki) / sqrt(coV(T, T)) * sqrt(coV(Ki, Ki))\n",
    "# uma métrica de cada vez"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
