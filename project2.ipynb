{
 "cells": [
  {
   "cell_type": "code",
   "id": "bce945e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T21:37:13.087376Z",
     "start_time": "2025-06-07T21:37:13.060007Z"
    }
   },
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from itertools import islice"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "9cd3943d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T21:37:13.982724Z",
     "start_time": "2025-06-07T21:37:13.972020Z"
    }
   },
   "source": [
    "directory = 'corpus2mw'"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "92a7b9e9",
   "metadata": {},
   "source": [
    "# Part I"
   ]
  },
  {
   "cell_type": "code",
   "id": "e71e29ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T21:37:15.198554Z",
     "start_time": "2025-06-07T21:37:15.188361Z"
    }
   },
   "source": [
    "def preprocess_text(text):\n",
    "    # normalize punctuation\n",
    "    text = text.replace(\"’\", \"'\").replace(\"‘\", \"'\").replace(\"−\", \"-\").replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "\n",
    "    # agregate numbers with commas like 4,000–7,000 → 4000_7000\n",
    "    text = re.sub(r'(\\d{1,3}(?:,\\d{3})*)\\s*[-–—]\\s*(\\d{1,3}(?:,\\d{3})*)',\n",
    "                  lambda m: m.group(1).replace(',', '') + '_' + m.group(2).replace(',', ''),\n",
    "                  text)\n",
    "\n",
    "    # remove commas in numbers like 9,500 → 9500\n",
    "    text = re.sub(r'(?<=\\d),(?=\\d)', '', text)\n",
    "\n",
    "    # remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # substitute hyphens between words with underscores like \"word-word\" → \"word_word\"\n",
    "    text = re.sub(r\"(?<=\\w)-(?=\\w)\", \"_\", text)\n",
    "\n",
    "    # remove characters that are not letters, numbers, spaces, or underscores\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s_áéíóúâêîôûàèìòùãõçÁÉÍÓÚÂÊÎÔÛÀÈÌÒÙÃÕÇ']\", ' ', text)\n",
    "\n",
    "    # normalize spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text.lower().strip()"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "de11d491",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T21:37:17.390518Z",
     "start_time": "2025-06-07T21:37:17.385516Z"
    }
   },
   "source": [
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text)"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "930a94d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T21:37:18.679452Z",
     "start_time": "2025-06-07T21:37:18.666312Z"
    }
   },
   "source": [
    "def extract_ngrams(tokens, max_n):\n",
    "    result = Counter()\n",
    "\n",
    "    for n in range(1, max_n + 1):\n",
    "        ngrams = zip(*(islice(tokens, i, None) for i in range(n)))\n",
    "        result.update(ngrams)\n",
    "\n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "ff1d83d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T21:37:19.904882Z",
     "start_time": "2025-06-07T21:37:19.890665Z"
    }
   },
   "source": [
    "def dice_score(ngram, freq, ngrams):\n",
    "    if len(ngram) < 2:\n",
    "        return 0\n",
    "\n",
    "    total = 0\n",
    "    for k in range(1, len(ngram)):\n",
    "        part1 = ngram[:k]\n",
    "        part2 = ngram[k:]\n",
    "        freq1 = ngrams.get(part1, 0)\n",
    "        freq2 = ngrams.get(part2, 0)\n",
    "        total += freq1 + freq2\n",
    "\n",
    "    if total == 0:\n",
    "        return 0\n",
    "\n",
    "    return (freq * 2) / (total / (len(ngram) - 1))"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "a08fe352",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T21:37:21.131134Z",
     "start_time": "2025-06-07T21:37:21.119136Z"
    }
   },
   "source": [
    "def scp_score(ngram, freq, ngrams):\n",
    "    if len(ngram) < 2:\n",
    "        return 0\n",
    "\n",
    "    total = 0\n",
    "    for k in range(1, len(ngram)):\n",
    "        part1 = ngram[:k]\n",
    "        part2 = ngram[k:]\n",
    "        freq1 = ngrams.get(part1, 0)\n",
    "        freq2 = ngrams.get(part2, 0)\n",
    "        total += freq1 * freq2\n",
    "\n",
    "    if total == 0:\n",
    "        return 0\n",
    "\n",
    "    return (freq ** 2) / (total / (len(ngram) - 1))"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "1f82f399",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T21:37:22.876787Z",
     "start_time": "2025-06-07T21:37:22.863779Z"
    }
   },
   "source": [
    "def mi_score(ngram, freq, ngrams, corpus_size):\n",
    "    if len(ngram) < 2:\n",
    "        return 0\n",
    "\n",
    "    total = 0\n",
    "    for k in range(1, len(ngram)):\n",
    "        part1 = ngram[:k]\n",
    "        part2 = ngram[k:]\n",
    "        freq1 = ngrams.get(part1, 0)\n",
    "        freq2 = ngrams.get(part2, 0)\n",
    "        total += (freq1 / corpus_size) * (freq2 / corpus_size)\n",
    "\n",
    "    if total == 0:\n",
    "        return 0\n",
    "\n",
    "    return math.log((freq / corpus_size) / (total / (len(ngram) - 1)))"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "794c4d84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T21:37:24.787723Z",
     "start_time": "2025-06-07T21:37:24.775211Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "    detect relevant ngrams that are local maxima\n",
    "    (those that have a score higher than the scores of their parts)\n",
    "\"\"\"\n",
    "def is_local_max(ngram, score, all_scores):\n",
    "    if len(ngram) < 2:\n",
    "        return False\n",
    "\n",
    "    for k in range(1, len(ngram)):\n",
    "        left = ngram[:k]\n",
    "        right = ngram[k:]\n",
    "        left_score = all_scores.get(left, 0)\n",
    "        right_score = all_scores.get(right, 0)\n",
    "\n",
    "        if score <= max(left_score, right_score):\n",
    "            return False\n",
    "\n",
    "    return True"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "223e0718",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T21:37:26.591490Z",
     "start_time": "2025-06-07T21:37:26.562395Z"
    }
   },
   "source": [
    "def extract_res(directory, num_files=None, specific_file=None, metric=\"scp\"):\n",
    "    files = sorted([f for f in os.listdir(directory) if f.startswith('fil_')],\n",
    "                  key=lambda x: int(x.split('_')[1]))\n",
    "    \n",
    "    if specific_file:\n",
    "        files = [specific_file] if specific_file in files else exit(f\"File {specific_file} not found in directory {directory}.\")\n",
    "    elif num_files:\n",
    "        files = files[:num_files]\n",
    "    \n",
    "    results = []\n",
    "    all_tokens = []\n",
    "    all_ngrams = Counter()\n",
    "    \n",
    "    for filename in tqdm(files, desc=\"Extracting REs\"):\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as f:\n",
    "            original = f.read()\n",
    "        \n",
    "        preprocessed = preprocess_text(original)\n",
    "        tokens = tokenize(preprocessed)\n",
    "        ngrams = extract_ngrams(tokens, max_n=7)  # up to 7 words per n-gram\n",
    "        \n",
    "        all_tokens.extend(tokens)\n",
    "        all_ngrams.update(ngrams)\n",
    "\n",
    "    dice_scores = {}\n",
    "    scp_scores = {}\n",
    "    mi_scores = {}\n",
    "\n",
    "    for ngram, freq in all_ngrams.items():\n",
    "        dice_scores[ngram] = dice_score(ngram, freq, all_ngrams)\n",
    "        scp_scores[ngram] = scp_score(ngram, freq, all_ngrams)\n",
    "        mi_scores[ngram] = mi_score(ngram, freq, all_ngrams, len(all_tokens))\n",
    "\n",
    "    local_max_dice_scores = [\n",
    "        (ngram, score) for ngram, score in dice_scores.items()\n",
    "        if is_local_max(ngram, score, dice_scores)\n",
    "    ]\n",
    "\n",
    "    local_max_scp_scores = [\n",
    "        (ngram, score) for ngram, score in scp_scores.items()\n",
    "        if is_local_max(ngram, score, scp_scores)\n",
    "    ]\n",
    "\n",
    "    local_max_mi_scores = [\n",
    "        (ngram, score) for ngram, score in mi_scores.items()\n",
    "        if is_local_max(ngram, score, mi_scores)\n",
    "    ]\n",
    "\n",
    "    results.append({\n",
    "        'dice_scores': sorted(dice_scores.items(), key=lambda x: x[1], reverse=True),\n",
    "        'local_max_dice_scores': sorted(local_max_dice_scores, key=lambda x: x[1], reverse=True),\n",
    "        'scp_scores': sorted(scp_scores.items(), key=lambda x: x[1], reverse=True),\n",
    "        'local_max_scp_scores': sorted(local_max_scp_scores, key=lambda x: x[1], reverse=True),\n",
    "        'mi_scores': sorted(mi_scores.items(), key=lambda x: x[1], reverse=True),\n",
    "        'local_max_mi_scores': sorted(local_max_mi_scores, key=lambda x: x[1], reverse=True)\n",
    "    })\n",
    "\n",
    "    return results"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "06d7b1d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T21:37:42.784838Z",
     "start_time": "2025-06-07T21:37:29.006709Z"
    }
   },
   "source": [
    "# process a specific file\n",
    "#results = process_scores_localmax(directory, specific_file='fil_2')[0]\n",
    "# or process a limited number of files\n",
    "results = extract_res(directory, num_files=100)[0]\n",
    "# or process all files\n",
    "#results = process_scores_localmax(directory)[0]\n",
    "\n",
    "print(\"Dice Scores:\")\n",
    "for ngram, score in results['dice_scores'][:10]:\n",
    "    print(f\"{' '.join(ngram)}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nLocal Max Dice Scores:\")\n",
    "for ngram, score in results['local_max_dice_scores'][:10]:\n",
    "    print(f\"{' '.join(ngram)}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nSCP Scores:\")\n",
    "for ngram, score in results['scp_scores'][:10]:\n",
    "    print(f\"{' '.join(ngram)}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nLocal Max SCP Scores:\")\n",
    "for ngram, score in results['local_max_scp_scores'][:10]:\n",
    "    print(f\"{' '.join(ngram)}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nMI Scores:\")\n",
    "for ngram, score in results['mi_scores'][:10]:\n",
    "    print(f\"{' '.join(ngram)}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nLocal Max MI Scores:\")\n",
    "for ngram, score in results['local_max_mi_scores'][:10]:\n",
    "    print(f\"{' '.join(ngram)}: {score:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting REs: 100%|██████████| 100/100 [00:00<00:00, 129.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice Scores:\n",
      "unaided eyesight: 1.0000\n",
      "mwene mbandu: 1.0000\n",
      "lyondthzi kapova: 1.0000\n",
      "dianhenga aspirante: 1.0000\n",
      "aspirante mjinji: 1.0000\n",
      "sultans murad: 1.0000\n",
      "brahmi alphabet: 1.0000\n",
      "urdu kashmiri: 1.0000\n",
      "laos cambodia: 1.0000\n",
      "ldc conferences: 1.0000\n",
      "\n",
      "Local Max Dice Scores:\n",
      "unaided eyesight: 1.0000\n",
      "mwene mbandu: 1.0000\n",
      "lyondthzi kapova: 1.0000\n",
      "dianhenga aspirante: 1.0000\n",
      "aspirante mjinji: 1.0000\n",
      "sultans murad: 1.0000\n",
      "brahmi alphabet: 1.0000\n",
      "urdu kashmiri: 1.0000\n",
      "laos cambodia: 1.0000\n",
      "ldc conferences: 1.0000\n",
      "\n",
      "SCP Scores:\n",
      "unaided eyesight: 1.0000\n",
      "mwene mbandu: 1.0000\n",
      "lyondthzi kapova: 1.0000\n",
      "dianhenga aspirante: 1.0000\n",
      "aspirante mjinji: 1.0000\n",
      "sultans murad: 1.0000\n",
      "brahmi alphabet: 1.0000\n",
      "urdu kashmiri: 1.0000\n",
      "laos cambodia: 1.0000\n",
      "ldc conferences: 1.0000\n",
      "\n",
      "Local Max SCP Scores:\n",
      "unaided eyesight: 1.0000\n",
      "mwene mbandu: 1.0000\n",
      "lyondthzi kapova: 1.0000\n",
      "dianhenga aspirante: 1.0000\n",
      "aspirante mjinji: 1.0000\n",
      "sultans murad: 1.0000\n",
      "brahmi alphabet: 1.0000\n",
      "urdu kashmiri: 1.0000\n",
      "laos cambodia: 1.0000\n",
      "ldc conferences: 1.0000\n",
      "\n",
      "MI Scores:\n",
      "unaided eyesight: 11.9372\n",
      "mwene mbandu: 11.9372\n",
      "lyondthzi kapova: 11.9372\n",
      "dianhenga aspirante: 11.9372\n",
      "aspirante mjinji: 11.9372\n",
      "sultans murad: 11.9372\n",
      "brahmi alphabet: 11.9372\n",
      "urdu kashmiri: 11.9372\n",
      "laos cambodia: 11.9372\n",
      "ldc conferences: 11.9372\n",
      "\n",
      "Local Max MI Scores:\n",
      "unaided eyesight: 11.9372\n",
      "mwene mbandu: 11.9372\n",
      "lyondthzi kapova: 11.9372\n",
      "dianhenga aspirante: 11.9372\n",
      "aspirante mjinji: 11.9372\n",
      "sultans murad: 11.9372\n",
      "brahmi alphabet: 11.9372\n",
      "urdu kashmiri: 11.9372\n",
      "laos cambodia: 11.9372\n",
      "ldc conferences: 11.9372\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "06450139",
   "metadata": {},
   "source": [
    "## Precision"
   ]
  },
  {
   "cell_type": "code",
   "id": "b28c4f63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T21:43:25.316738Z",
     "start_time": "2025-06-07T21:43:25.312504Z"
    }
   },
   "source": [
    "def precision_evaluation(candidates, sample_size):\n",
    "    return random.sample(candidates, min(sample_size, len(candidates)))"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "bebf45fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T21:43:26.275920Z",
     "start_time": "2025-06-07T21:43:26.258490Z"
    }
   },
   "source": [
    "precision_sample_dice = precision_evaluation(results['local_max_dice_scores'], 50)\n",
    "precision_sample_scp = precision_evaluation(results['local_max_scp_scores'], 50)\n",
    "precision_sample_mi = precision_evaluation(results['local_max_mi_scores'], 50)\n",
    "\n",
    "with open('precision_sample_dice.txt', 'w', encoding='utf-8') as f:\n",
    "    for i, (ngram, score) in enumerate(precision_sample_dice, 1):\n",
    "        phrase = ' '.join(ngram)\n",
    "        f.write(f\"{phrase} | Score: {score:.4f} | Annotation: TP / FP\\n\")\n",
    "\n",
    "with open('precision_sample_scp.txt', 'w', encoding='utf-8') as f:\n",
    "    for i, (ngram, score) in enumerate(precision_sample_scp, 1):\n",
    "        phrase = ' '.join(ngram)\n",
    "        f.write(f\"{phrase} | Score: {score:.4f} | Annotation: TP / FP\\n\")\n",
    "\n",
    "with open('precision_sample_mi.txt', 'w', encoding='utf-8') as f:\n",
    "    for i, (ngram, score) in enumerate(precision_sample_mi, 1):\n",
    "        phrase = ' '.join(ngram)\n",
    "        f.write(f\"{phrase} | Score: {score:.4f} | Annotation: TP / FP\\n\")\n",
    "\n",
    "# after the creation of the precision sample file we need to edit it manually\n",
    "# writting TP (valid) or FP (not valid) for each relevant expression"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "fac215fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T21:47:33.677860Z",
     "start_time": "2025-06-07T21:47:33.664757Z"
    }
   },
   "source": [
    "def calculate_precision(file):\n",
    "    valid = 0\n",
    "    total = 0\n",
    "    \n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip().endswith('TP'):\n",
    "                valid += 1\n",
    "            total += 1\n",
    "    \n",
    "    return valid / total if total > 0 else 0"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "c0d2fe6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T21:47:35.041417Z",
     "start_time": "2025-06-07T21:47:35.028252Z"
    }
   },
   "source": [
    "print(\"Dice Precision:\", calculate_precision(\"precision_sample_dice.txt\"))\n",
    "print(\"SCP Precision:\", calculate_precision(\"precision_sample_scp.txt\"))\n",
    "print(\"MI Precision:\", calculate_precision(\"precision_sample_mi.txt\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice Precision: 0.36\n",
      "SCP Precision: 0.32\n",
      "MI Precision: 0.46\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "id": "493446ec",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2841aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paragraphs(directory, num_paragraphs, max_files=None):\n",
    "    files = sorted([f for f in os.listdir(directory) if f.startswith('fil_')],\n",
    "                  key=lambda x: int(x.split('_')[1]))\n",
    "    \n",
    "    if max_files:\n",
    "        files = files[:max_files]\n",
    "    \n",
    "    all_paragraphs = []\n",
    "    \n",
    "    for filename in files:\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "        all_paragraphs.extend(paragraphs)\n",
    "    \n",
    "    return random.sample(all_paragraphs, min(num_paragraphs, len(all_paragraphs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "116ae739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph 1:\n",
      "ENSO may be linked to civil conflicts. Scientists at The Earth Institute of Columbia University, having analyzed data from 1950 to 2004, suggest ENSO may have had a role in 21% of all civil conflicts since 1950, with the risk of annual civil conflict doubling from 3% to 6% in countries affected by ENSO during El Niño years relative to La Niña years.\n",
      "Thus it is even smaller than the next correction term formula_31 of Stirling's formula.\n",
      "==================================================\n",
      "\n",
      "Paragraph 2:\n",
      "The blue crane (\"Anthropoides paradiseus\"), also known as the Stanley crane and the paradise crane, is the national bird of South Africa.\n",
      "Note that phenolic resin products are apt to swell slightly if they are used in areas that are perpetually damp. Varnishing the product helps to prevent this.\n",
      "In the mid-2000s (decade) Juan Francisco Casas generated Internet attention for a series of large-scale, photo-realistic ballpoint duplications of his own snapshots of friends, utilising only blue pens.\n",
      "That said, overlap of sexual preference disorders and the practice of BDSM practices can occur.\n",
      "The largest sport hub in the city is Tofig Bahramov Stadium with 31,200 seating capacity. The city's three main football clubs are Neftchi Baku, FC Baku and Inter Baku of whom first has eight Premier League titles making Neftchi the most successful Azerbaijani football club. Baku also has several football clubs in the premier and regional leagues, including AZAL in Premier League. The Baku Olympic Stadium, a planned 65,000 seat football stadium, is currently under construction and will be based on Boyuk Shor settlement.\n",
      "To the south, HMS \"Bellerophon\" was in serious trouble as the huge broadside of \"Orient\" pounded the ship. At 19:50 the mizen mast and main mast both collapsed and fires broke out simultaneously at several points. Although the blazes were extinguished, the ship had suffered more than 200 casualties. Captain Darby recognised that his position was untenable and ordered the anchor cables cut at 20:20. The battered ship drifted away from the battle under continued fire from \"Tonnant\" as the foremast collapsed as well. \"Orient\" had also suffered significant damage and Admiral Brueys had been struck in the midriff by a cannonball that almost cut him in half. He died fifteen minutes later, remaining on deck and refusing to be carried below. \"Orient\"'s captain, Luc-Julien-Joseph Casabianca, was also wounded, struck in the face by flying debris and knocked unconscious, while his twelve-year-old son had a leg torn off by a cannonball as he stood beside his father. The most southerly British ship, \"Majestic\", had become briefly entangled with the 80-gun \"Tonnant\", and in the resulting battle, suffered heavy casualties. Captain George Blagdon Westcott was among the dead, killed by French musket fire. Lieutenant Robert Cuthbert assumed command and successfully disentangled his ship, allowing the badly damaged \"Majestic\" to drift further southwards so that by 20:30 it was stationed between \"Tonnant\" and the next in line, \"Heureux\", engaging both. To support the centre, Captain Thompson of \"Leander\" abandoned the futile efforts to drag the stranded \"Culloden\" off the shoal and sailed down the embattled French line, entering the gap created by the drifting \"Peuple Souverain\" and opening a fierce raking fire on \"Franklin\" and \"Orient\".\n",
      "==================================================\n",
      "\n",
      "Paragraph 3:\n",
      "Similar organisations in other countries.\n",
      "In approving the Balfour Declaration, Leopold Amery, one of the Secretaries to the British War Cabinet of 1917–18, testified under oath to the Anglo-American Committee of Inquiry in January 1946 from his personal knowledge that:\n",
      "Confederate retreat.\n",
      "During May–June 1991, Euronymous of Mayhem opened an independent record shop named \"Helvete\" (Norwegian for 'hell') in Oslo. It quickly became the focal point of Norway's emerging black metal scene and a meeting place for many of its musicians; especially the members of Mayhem, Burzum, Emperor and Thorns. Jon 'Metalion' Kristiansen, writer of the fanzine \"Slayer\", said that the opening of Helvete was \"the creation of the whole Norwegian Black Metal scene\". In its basement, Euronymous founded an independent record label named Deathlike Silence Productions. With the rising popularity of his band and others like it, the underground success of Euronymous's label is often credited for encouraging other record labels, who had previously shunned black metal acts, to then reconsider and release their material.\n",
      "Basel is located at the meeting point of France, Germany and Switzerland; because it is so near other countries and is beyond the Jura Mountains, many within the Swiss military reportedly believe that the city is indefensible during wartime. It has numerous road and rail crossings between Switzerland and the other two countries. With Switzerland joining the Schengen Area on December 12, 2008, immigration checks were no longer carried out at the crossings. However, Switzerland did not join the EU customs regime and customs checks are still conducted at or near the crossings.\n",
      "All team members, both officer and enlisted, pilots and staff officers, come from the ranks of regular Navy and United States Marine Corps units. The demonstration pilots and narrator are made up of Navy and USMC Naval Aviators. Pilots serve two to three years, and position assignments are made according to team needs, pilot experience levels, and career considerations for members.\n",
      "Bixby was executive producer of the three \"Hulk\" made-for-television sequel movies in the late 1980s and in 1990. He also directed the latter two.\n",
      "==================================================\n",
      "\n",
      "Paragraph 4:\n",
      "Possible reflexes in Insular Celtic.\n",
      "Chimpanzees use a variety of facial expressions, postures and sounds to communicate with each other. Chimps have expressive faces which are important in close-up communications. When frightened, a \"full closed grin\" causes nearby individuals to be fearful, as well. Other facial expressions include the \"lip flip\", \"pout\", \"sneer\", and \"compressed-lips face\". When submitting to a conspecific, a chimp will crunch, bob and extend a hand. When in an aggressive mode, a chimp will swagger bipedally, hunched over and arms waving, in an attempt to exaggerate its size. Chimps will beat their hands and feet against the trunks of large tree, an act known as \"drumming\".\n",
      "because each of the basis vectors is a constant.\n",
      "Rhodesia.\n",
      "In 1977, Giorgio Moroder again became responsible for a development in disco. Alongside Donna Summer and Pete Bellotte he wrote the song \"I Feel Love\" for Summer to perform. It became the first well-known disco hit to have a completely synthesised backing track. The song is still considered to have been well ahead of its time. Other disco producers, most famously Tom Moulton, grabbed ideas and techniques from dub music (which came with the increased Jamaican migration to New York City in the seventies) to provide alternatives to the four on the floor style that dominated. Larry Levan utilized style keys from dub and jazz and more as one of the most successful remixers of all time to create early versions of house music that sparked the genre.\n",
      "Even from this fleeting moment of independence democracy began to unravel. On 5 July 1960 a military mutiny by Congolese soldiers against their European officers broke out in the capital and rampant looting began. On 11 July 1960 the richest province of the country, Katanga, seceded under Moise Tshombe. The United Nations sent 20,000 peacekeepers to protect Europeans in the country and try to restore order. Western paramilitaries and mercenaries, often hired by mining companies to protect their interests, also began to pour into the country. In this same period Congo's second richest province, Kasai, also announced its independence on 8 August 1960.\n",
      "Djiboutian Navy.\n",
      "Krāslava, Daugavpils, Līvāni, Jēkabpils, Pļaviņas, Aizkraukle, Jaunjelgava, Lielvārde, Kegums, Ogre, Ikšķile, Salaspils and Riga.\n",
      "History.\n",
      "==================================================\n",
      "\n",
      "Paragraph 3:\n",
      "Possible reflexes in Insular Celtic.\n",
      "Chimpanzees use a variety of facial expressions, postures and sounds to communicate with each other. Chimps have expressive faces which are important in close-up communications. When frightened, a \"full closed grin\" causes nearby individuals to be fearful, as well. Other facial expressions include the \"lip flip\", \"pout\", \"sneer\", and \"compressed-lips face\". When submitting to a conspecific, a chimp will crunch, bob and extend a hand. When in an aggressive mode, a chimp will swagger bipedally, hunched over and arms waving, in an attempt to exaggerate its size. Chimps will beat their hands and feet against the trunks of large tree, an act known as \"drumming\".\n",
      "because each of the basis vectors is a constant.\n",
      "Rhodesia.\n",
      "In 1977, Giorgio Moroder again became responsible for a development in disco. Alongside Donna Summer and Pete Bellotte he wrote the song \"I Feel Love\" for Summer to perform. It became the first well-known disco hit to have a completely synthesised backing track. The song is still considered to have been well ahead of its time. Other disco producers, most famously Tom Moulton, grabbed ideas and techniques from dub music (which came with the increased Jamaican migration to New York City in the seventies) to provide alternatives to the four on the floor style that dominated. Larry Levan utilized style keys from dub and jazz and more as one of the most successful remixers of all time to create early versions of house music that sparked the genre.\n",
      "Even from this fleeting moment of independence democracy began to unravel. On 5 July 1960 a military mutiny by Congolese soldiers against their European officers broke out in the capital and rampant looting began. On 11 July 1960 the richest province of the country, Katanga, seceded under Moise Tshombe. The United Nations sent 20,000 peacekeepers to protect Europeans in the country and try to restore order. Western paramilitaries and mercenaries, often hired by mining companies to protect their interests, also began to pour into the country. In this same period Congo's second richest province, Kasai, also announced its independence on 8 August 1960.\n",
      "Djiboutian Navy.\n",
      "Krāslava, Daugavpils, Līvāni, Jēkabpils, Pļaviņas, Aizkraukle, Jaunjelgava, Lielvārde, Kegums, Ogre, Ikšķile, Salaspils and Riga.\n",
      "History.\n",
      "==================================================\n",
      "\n",
      "Paragraph 4:\n",
      "Similar organisations in other countries.\n",
      "In approving the Balfour Declaration, Leopold Amery, one of the Secretaries to the British War Cabinet of 1917–18, testified under oath to the Anglo-American Committee of Inquiry in January 1946 from his personal knowledge that:\n",
      "Confederate retreat.\n",
      "During May–June 1991, Euronymous of Mayhem opened an independent record shop named \"Helvete\" (Norwegian for 'hell') in Oslo. It quickly became the focal point of Norway's emerging black metal scene and a meeting place for many of its musicians; especially the members of Mayhem, Burzum, Emperor and Thorns. Jon 'Metalion' Kristiansen, writer of the fanzine \"Slayer\", said that the opening of Helvete was \"the creation of the whole Norwegian Black Metal scene\". In its basement, Euronymous founded an independent record label named Deathlike Silence Productions. With the rising popularity of his band and others like it, the underground success of Euronymous's label is often credited for encouraging other record labels, who had previously shunned black metal acts, to then reconsider and release their material.\n",
      "Basel is located at the meeting point of France, Germany and Switzerland; because it is so near other countries and is beyond the Jura Mountains, many within the Swiss military reportedly believe that the city is indefensible during wartime. It has numerous road and rail crossings between Switzerland and the other two countries. With Switzerland joining the Schengen Area on December 12, 2008, immigration checks were no longer carried out at the crossings. However, Switzerland did not join the EU customs regime and customs checks are still conducted at or near the crossings.\n",
      "All team members, both officer and enlisted, pilots and staff officers, come from the ranks of regular Navy and United States Marine Corps units. The demonstration pilots and narrator are made up of Navy and USMC Naval Aviators. Pilots serve two to three years, and position assignments are made according to team needs, pilot experience levels, and career considerations for members.\n",
      "Bixby was executive producer of the three \"Hulk\" made-for-television sequel movies in the late 1980s and in 1990. He also directed the latter two.\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paragraphs_sample = extract_paragraphs(directory, 4, max_files=5)\n",
    "\n",
    "for i, paragraph in enumerate(paragraphs_sample, 1):\n",
    "    print(f\"Paragraph {i}:\\n{paragraph}\\n{'='*50}\\n\")\n",
    "\n",
    "# after getting the paragraphs sample, we need to create the file recall_sample.txt\n",
    "# and write the relevant expressions found in each paragraph manually"
   ]
  },
  {
   "cell_type": "code",
   "id": "9f62df85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T21:55:09.935794Z",
     "start_time": "2025-06-07T21:55:09.922797Z"
    }
   },
   "source": [
    "def calculate_recall(file, candidates):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        manual_expressions = set(line.strip() for line in f if line.strip())\n",
    "    \n",
    "    candidates_expressions = set(' '.join(ngram) for ngram, _ in candidates)\n",
    "    \n",
    "    matches = 0\n",
    "    for expression in manual_expressions:\n",
    "        if expression.lower() in candidates_expressions:\n",
    "            matches += 1\n",
    "    \n",
    "    return matches / len(manual_expressions) if manual_expressions else 0"
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "id": "fc55ddf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T21:55:37.109734Z",
     "start_time": "2025-06-07T21:55:36.485409Z"
    }
   },
   "source": [
    "print(\"Recall (Dice):\", calculate_recall(\"recall_sample.txt\", results[\"local_max_dice_scores\"]))\n",
    "print(\"Recall (SCP):\", calculate_recall(\"recall_sample.txt\", results[\"local_max_scp_scores\"]))\n",
    "print(\"Recall (MI):\", calculate_recall(\"recall_sample.txt\", results[\"local_max_mi_scores\"]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall (Dice): 0.33613445378151263\n",
      "Recall (SCP): 0.35294117647058826\n",
      "Recall (MI): 0.35294117647058826\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "id": "7aaa3649",
   "metadata": {},
   "source": [
    "# Part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbed5b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_explicit_keywords_per_doc(directory, num_files=None, top_n=15):\n",
    "    files = sorted([f for f in os.listdir(directory) if f.startswith('fil_')],\n",
    "                   key=lambda x: int(x.split('_')[1]))\n",
    "    \n",
    "    if num_files:\n",
    "        files = files[:num_files]\n",
    "\n",
    "    explicit_keywords = []\n",
    "\n",
    "    for filename in files:\n",
    "        results = extract_res(directory, specific_file=filename)[0]\n",
    "        local_max_scores = results['local_max_scp_scores']\n",
    "        top_terms = [ngram for ngram, score in local_max_scores[:top_n]]\n",
    "        explicit_keywords.append(top_terms)\n",
    "\n",
    "    return explicit_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b085eb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_frequencies(directory, num_files=None):\n",
    "    files = sorted([f for f in os.listdir(directory) if f.startswith('fil_')],\n",
    "                   key=lambda x: int(x.split('_')[1]))\n",
    "    \n",
    "    if num_files:\n",
    "        files = files[:num_files]\n",
    "\n",
    "    freqs = []\n",
    "    total_tokens = []\n",
    "\n",
    "    for filename in files:\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        preprocessed = preprocess_text(text)\n",
    "        tokens = tokenize(preprocessed)\n",
    "        total_tokens.append(len(tokens))\n",
    "\n",
    "        ngrams = extract_ngrams(tokens, max_n=7)\n",
    "        freqs.append(ngrams)\n",
    "\n",
    "    return freqs, total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43106599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate probabilities P(T,d) and P(T,D)\n",
    "def calculate_probabilities(freqs, total_tokens):\n",
    "    num_docs = len(freqs)\n",
    "    all_terms = set()\n",
    "    for doc_freq in freqs:\n",
    "        all_terms.update(doc_freq.keys())\n",
    "\n",
    "    P_T_d = [{} for _ in range(num_docs)]\n",
    "    P_T_D = {}\n",
    "\n",
    "    for d, doc_freq in enumerate(freqs):\n",
    "        total = total_tokens[d]\n",
    "        for term in all_terms:\n",
    "            freq = doc_freq.get(term, 0)\n",
    "            P_T_d[d][term] = freq / total if total > 0 else 0\n",
    "\n",
    "    for term in all_terms:\n",
    "        sum_p = sum(P_T_d[d][term] for d in range(num_docs))\n",
    "        P_T_D[term] = sum_p / num_docs\n",
    "\n",
    "    return P_T_d, P_T_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c660414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical correlation between terms\n",
    "def covariance(term1, term2, P_T_d, P_T_D):\n",
    "    num_docs = len(P_T_d)\n",
    "    s = 0\n",
    "    for d in range(num_docs):\n",
    "        s += (P_T_d[d].get(term1, 0) - P_T_D.get(term1, 0)) * (P_T_d[d].get(term2, 0) - P_T_D.get(term2, 0))\n",
    "    return s / num_docs\n",
    "\n",
    "def correlation(term1, term2, P_T_d, P_T_D):\n",
    "    cov_12 = covariance(term1, term2, P_T_d, P_T_D)\n",
    "    cov_11 = covariance(term1, term1, P_T_d, P_T_D)\n",
    "    cov_22 = covariance(term2, term2, P_T_d, P_T_D)\n",
    "\n",
    "    denom = math.sqrt(cov_11) * math.sqrt(cov_22)\n",
    "    if denom == 0:\n",
    "        return 0\n",
    "    return cov_12 / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0eb64399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# score and rank implicit terms by correlation to explicit ones\n",
    "def calculate_implicit_scores(explicit_keywords, P_T_d, P_T_D, min_doc_occurrences=2):\n",
    "    num_docs = len(explicit_keywords)\n",
    "    implicit_scores_per_doc = []\n",
    "\n",
    "    all_terms = list(P_T_D.keys())\n",
    "    candidate_terms = [\n",
    "        term for term in all_terms\n",
    "        if sum(1 for d in range(num_docs) if P_T_d[d].get(term, 0) > 0) >= min_doc_occurrences\n",
    "    ]\n",
    "\n",
    "    for d in tqdm(range(num_docs), desc=\"Calculating Implicits\"):\n",
    "        explicit = set(explicit_keywords[d])\n",
    "        scores = {}\n",
    "\n",
    "        for T in candidate_terms:\n",
    "            if T in explicit:\n",
    "                continue\n",
    "\n",
    "            corr_sum = 0\n",
    "            count = 0\n",
    "\n",
    "            for K in explicit:\n",
    "                c = correlation(T, K, P_T_d, P_T_D)\n",
    "                corr_sum += c\n",
    "                count += 1\n",
    "\n",
    "            scores[T] = corr_sum / count if count > 0 else 0\n",
    "\n",
    "        implicit_scores_per_doc.append(scores)\n",
    "\n",
    "    return implicit_scores_per_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "805aa41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting REs:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting REs: 100%|██████████| 1/1 [00:00<00:00,  8.44it/s]\n",
      "Extracting REs: 100%|██████████| 1/1 [00:00<00:00, 486.58it/s]\n",
      "Extracting REs: 100%|██████████| 1/1 [00:00<00:00, 58.36it/s]\n",
      "Extracting REs: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Extracting REs: 100%|██████████| 1/1 [00:00<00:00, 58.10it/s]\n",
      "Extracting REs: 100%|██████████| 1/1 [00:00<00:00, 100.93it/s]\n",
      "Extracting REs: 100%|██████████| 1/1 [00:00<00:00, 37.52it/s]\n",
      "Extracting REs: 100%|██████████| 1/1 [00:00<00:00, 56.55it/s]\n",
      "Extracting REs: 100%|██████████| 1/1 [00:00<00:00, 102.22it/s]\n",
      "Extracting REs: 100%|██████████| 1/1 [00:00<00:00, 110.16it/s]\n",
      "Calculating Implicits: 100%|██████████| 10/10 [00:05<00:00,  1.67it/s]\n"
     ]
    }
   ],
   "source": [
    "explicit_keywords = extract_explicit_keywords_per_doc(directory, num_files=10, top_n=10)\n",
    "\n",
    "freqs, total_tokens = get_term_frequencies(directory, num_files=10)\n",
    "\n",
    "P_T_d, P_T_D = calculate_probabilities(freqs, total_tokens)\n",
    "\n",
    "implicit_scores = calculate_implicit_scores(explicit_keywords, P_T_d, P_T_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeaac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8070c4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Explicit Expressions (Doc 1):\n",
      "christian scribes\n",
      "crucial role\n",
      "language manuscripts\n",
      "comment extensively\n",
      "stands out\n",
      "fundamental critique\n",
      "ephesus reappears\n",
      "apparently sponsored\n",
      "anna comnena\n",
      "codes represent\n"
     ]
    }
   ],
   "source": [
    "print(f\"Top Explicit Expressions (Doc {document}):\")\n",
    "\n",
    "for term in explicit_keywords[document-1]:\n",
    "    print(' '.join(term) if isinstance(term, tuple) else term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b3b325a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Implicit Expressions (Doc 1):\n",
      "\"ii\" (score=0.985)\n",
      "\"instruments\" (score=0.980)\n",
      "\"users\" (score=0.945)\n",
      "\"rate\" (score=0.945)\n",
      "\"the title\" (score=0.943)\n",
      "\"the beginning\" (score=0.919)\n",
      "\"the beginning of\" (score=0.919)\n",
      "\"beginning of\" (score=0.919)\n",
      "\"hydrogen\" (score=0.916)\n",
      "\"removal\" (score=0.916)\n"
     ]
    }
   ],
   "source": [
    "top_implicit = sorted(implicit_scores[document-1].items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "print(f\"Top Implicit Expressions (Doc {document}):\")\n",
    "\n",
    "for term, score in top_implicit:\n",
    "    expression = \" \".join(term)\n",
    "    print(f'\"{expression}\" (score={score:.3f})')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
