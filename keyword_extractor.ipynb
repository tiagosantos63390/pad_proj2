{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be72bb57",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f70b1356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccae173a",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ca402f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a)\n",
    "def preprocess_text(text):\n",
    "    # Add spaces around special characters\n",
    "    text = re.sub(r'([;,!?<>()\\[\\]&])', r' \\1 ', text)\n",
    "    # Convert to lowercase and split into tokens\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# d)\n",
    "def get_ngrams(tokens, n):\n",
    "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c518bfc4",
   "metadata": {},
   "source": [
    "**Four Cohesion Metrics:**\n",
    "\n",
    "- SCP (Symmetrical Conditional Probability) - measures how likely the components co-occur compared to their independent occurrences\n",
    "\n",
    "- Dice - Measures the overlap between components\n",
    "\n",
    "- φ² (Phi-squared) - Chi-square based association measure\n",
    "\n",
    "- MI (Mutual Information) - Information-theoretic measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02dca6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c)\n",
    "def calculate_scp(n_gram, n_gram_freq, sub_gram_freqs):\n",
    "    numerator = n_gram_freq ** 2\n",
    "    denominator = sum(f1 * f2 for f1, f2 in sub_gram_freqs) / len(sub_gram_freqs)\n",
    "    return numerator / denominator if denominator!=0 else 0\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "def calculate_dice(n_gram, n_gram_freq, sub_gram_freqs):\n",
    "    if len(sub_gram_freqs) == 0:\n",
    "        return 0\n",
    "    \n",
    "    sum_dice = 0\n",
    "    for f1, f2 in sub_gram_freqs:\n",
    "        sum_dice += (2 * n_gram_freq) / (f1 + f2) if (f1 + f2) else 0\n",
    "    \n",
    "    return sum_dice / len(sub_gram_freqs)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "def calculate_phi_squared(n_gram, n_gram_freq, sub_gram_freqs, total_ngrams):\n",
    "    if len(sub_gram_freqs) == 0:\n",
    "        return 0\n",
    "    \n",
    "    sum_phi = 0\n",
    "    for f1, f2 in sub_gram_freqs:\n",
    "        N = total_ngrams\n",
    "        numerator = (N * n_gram_freq - f1 * f2) ** 2\n",
    "        denominator = f1 * f2 * (N - f1) * (N - f2) if (f1 * f2 * (N - f1) * (N - f2)) else 1\n",
    "        sum_phi += numerator / denominator\n",
    "    \n",
    "    return sum_phi / len(sub_gram_freqs)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "def calculate_mi(n_gram, n_gram_freq, sub_gram_freqs, total_ngrams):\n",
    "    if len(sub_gram_freqs) == 0:\n",
    "        return 0\n",
    "    \n",
    "    sum_mi = 0\n",
    "    for f1, f2 in sub_gram_freqs:\n",
    "        if f1 == 0 or f2 == 0:\n",
    "            continue\n",
    "        mi = math.log2((n_gram_freq * total_ngrams) / (f1 * f2))\n",
    "        sum_mi += mi\n",
    "    \n",
    "    return sum_mi / len(sub_gram_freqs) if len(sub_gram_freqs) > 0 else 0\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "def calculate_cohesion_score(ngram, freq, sub_pairs, total_ngrams, metric):\n",
    "    if metric == 'scp':\n",
    "        return calculate_scp(ngram, freq, sub_pairs)\n",
    "    elif metric == 'dice':\n",
    "        return calculate_dice(ngram, freq, sub_pairs)\n",
    "    elif metric == 'phi_squared':\n",
    "        return calculate_phi_squared(ngram, freq, sub_pairs, total_ngrams)\n",
    "    elif metric == 'mi':\n",
    "        return calculate_mi(ngram, freq, sub_pairs, total_ngrams)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid metric. Please choose from 'scp', 'dice', 'phi_squared'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f64afd6",
   "metadata": {},
   "source": [
    "- filtrar freq apenas no fim\n",
    "- procurar ngramas desde 7 (max) para baixo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe55d297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relevant_expressions(corpus_path, max_n=7, min_freq=2, metric='scp'):\n",
    "    # Count all ngrams first\n",
    "    ngram_counts = defaultdict(int)\n",
    "    for root, _, files in os.walk(corpus_path):\n",
    "        for file in files:\n",
    "            with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                tokens = preprocess_text(text)\n",
    "                for n in range(2, max_n+1):\n",
    "                    for ngram in get_ngrams(tokens, n):\n",
    "                        ngram_counts[ngram] += 1\n",
    "\n",
    "    ngram_counts = {k: v for k, v in ngram_counts.items() if v >= min_freq}\n",
    "\n",
    "    # Build n+1 lookup structure\n",
    "    nplus1_lookup = defaultdict(list)\n",
    "    for ngram in ngram_counts:\n",
    "        words = ngram.split()\n",
    "        if len(words) < max_n:\n",
    "            # Create both possible extensions\n",
    "            nplus1_lookup['* ' + ngram].append(ngram)\n",
    "            nplus1_lookup[ngram + ' *'].append(ngram)\n",
    "\n",
    "    # Calculate cohesion scores\n",
    "    cohesion_scores = {}\n",
    "    for ngram, freq in ngram_counts.items():\n",
    "        words = ngram.split()\n",
    "        if len(words) == 2:\n",
    "            f_x = ngram_counts.get(words[0], 0)\n",
    "            f_y = ngram_counts.get(words[1], 0)\n",
    "            cohesion_scores[ngram] = (freq ** 2) / (f_x * f_y) if (f_x * f_y) else 0\n",
    "        else:\n",
    "            sub_pairs = []\n",
    "            for i in range(1, len(words)):\n",
    "                left = ' '.join(words[:i])\n",
    "                right = ' '.join(words[i:])\n",
    "                sub_pairs.append((ngram_counts.get(left, 0), ngram_counts.get(right, 0)))\n",
    "            cohesion_scores[ngram] = calculate_cohesion_score(ngram, freq, sub_pairs, len(ngram_counts), metric)\n",
    "\n",
    "    # Optimized LocalMaxs check\n",
    "    relevant_expr = []\n",
    "    for ngram in cohesion_scores:\n",
    "        words = ngram.split()\n",
    "        current_score = cohesion_scores[ngram]\n",
    "        is_local_max = True\n",
    "\n",
    "        # Check (n-1)-grams\n",
    "        if len(words) > 2:\n",
    "            for i in range(len(words)):\n",
    "                sub_ngram = ' '.join(words[:i] + words[i+1:])\n",
    "                if cohesion_scores.get(sub_ngram, 0) >= current_score:\n",
    "                    is_local_max = False\n",
    "                    break\n",
    "\n",
    "        # Optimized (n+1)-gram check using lookup\n",
    "        if is_local_max and len(words) < max_n:\n",
    "            for pattern in ['* ' + ngram, ngram + ' *']:\n",
    "                for parent in nplus1_lookup.get(pattern, []):\n",
    "                    if cohesion_scores.get(parent, 0) >= current_score:\n",
    "                        is_local_max = False\n",
    "                        break\n",
    "                if not is_local_max:\n",
    "                    break\n",
    "\n",
    "        if is_local_max:\n",
    "            relevant_expr.append((ngram, cohesion_scores[ngram], ngram_counts[ngram]))\n",
    "\n",
    "    return sorted(relevant_expr, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "436af9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing corpus2mw...\n",
      "\n",
      "Top 10 Relevant Expressions from corpus2mw:\n",
      "1. is assisted in his her duties by (score: 1.50, frequency: 2)\n",
      "2. running british science fiction television series doctor (score: 1.50, frequency: 4)\n",
      "3. county greater poland voivodeship in west central (score: 1.50, frequency: 6)\n",
      "4. population belonged to no church are agnostic (score: 1.50, frequency: 3)\n",
      "5. see http en wikipedia org wiki wikipedia (score: 1.50, frequency: 2)\n",
      "6. fte basis for a student teacher ratio (score: 1.50, frequency: 2)\n",
      "7. starlings are small to medium sized passerine (score: 1.50, frequency: 3)\n",
      "8. gregarious their preferred habitat is fairly open (score: 1.50, frequency: 3)\n",
      "9. viewers votes in the sing off they (score: 1.50, frequency: 2)\n",
      "10. leaf string is in the language 0 (score: 1.50, frequency: 2)\n",
      "\n",
      "Processing corpus4mw...\n",
      "\n",
      "Top 10 Relevant Expressions from corpus4mw:\n",
      "1. represented northern illinois university the huskies competed (score: 1.50, frequency: 2)\n",
      "2. four doubles titles on the itf tour (score: 1.50, frequency: 2)\n",
      "3. from md 99 at st johns lane (score: 1.50, frequency: 2)\n",
      "4. government co educational primary p 7 school (score: 1.50, frequency: 2)\n",
      "5. nahiyah is a syrian nahiyah subdistrict located (score: 1.50, frequency: 2)\n",
      "6. critics at metacritic which assigns a normalised (score: 1.50, frequency: 2)\n",
      "7. see http en wikipedia org wiki wikipedia (score: 1.50, frequency: 2)\n",
      "8. expensive local services such as education social (score: 1.50, frequency: 2)\n",
      "9. education social services libraries main roads public (score: 1.50, frequency: 2)\n",
      "10. policing and fire services trading standards waste (score: 1.50, frequency: 2)\n"
     ]
    }
   ],
   "source": [
    "corpus_files = ['corpus2mw', 'corpus4mw']\n",
    "    \n",
    "# Process both corpora files\n",
    "for corpus in corpus_files:\n",
    "    print(f\"\\nProcessing {corpus}...\")\n",
    "    rexpr = extract_relevant_expressions(corpus)\n",
    "    \n",
    "    # Print top 10 relevant expressions\n",
    "    print(f\"\\nTop 10 Relevant Expressions from {corpus}:\")\n",
    "    for i, (ngram, score, freq) in enumerate(rexpr[:10], 1):\n",
    "        print(f\"{i}. {ngram} (score: {score:.2f}, frequency: {freq})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "605ae4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing corpus2mw...\n",
      "\n",
      "Top 10 Relevant Expressions from corpus2mw:\n",
      "1. is assisted in his her duties by (score: 1.33, frequency: 2)\n",
      "2. see http en wikipedia org wiki wikipedia (score: 1.33, frequency: 2)\n",
      "3. fte basis for a student teacher ratio (score: 1.33, frequency: 2)\n",
      "4. starlings are small to medium sized passerine (score: 1.33, frequency: 3)\n",
      "5. gregarious their preferred habitat is fairly open (score: 1.33, frequency: 3)\n",
      "6. viewers votes in the sing off they (score: 1.33, frequency: 2)\n",
      "7. 21 occupation of goldsboro march 24 advance (score: 1.33, frequency: 2)\n",
      "8. 24 advance on raleigh april 10 14 (score: 1.33, frequency: 2)\n",
      "9. 14 occupation of raleigh april 14 bennett (score: 1.33, frequency: 2)\n",
      "10. army march to washington d c via (score: 1.33, frequency: 2)\n",
      "\n",
      "Processing corpus4mw...\n",
      "\n",
      "Top 10 Relevant Expressions from corpus4mw:\n",
      "1. represented northern illinois university the huskies competed (score: 1.33, frequency: 2)\n",
      "2. four doubles titles on the itf tour (score: 1.33, frequency: 2)\n",
      "3. from md 99 at st johns lane (score: 1.33, frequency: 2)\n",
      "4. see http en wikipedia org wiki wikipedia (score: 1.33, frequency: 2)\n",
      "5. expensive local services such as education social (score: 1.33, frequency: 2)\n",
      "6. education social services libraries main roads public (score: 1.33, frequency: 2)\n",
      "7. total swiss population change in 2008 from (score: 1.33, frequency: 4)\n",
      "8. 000 items to its 8 000 cardholders (score: 1.33, frequency: 2)\n",
      "9. carnegie medal from the library association recognising (score: 1.33, frequency: 2)\n",
      "10. fulfils this condition is called a mustati (score: 1.33, frequency: 2)\n"
     ]
    }
   ],
   "source": [
    "# Now with other cohesion scores\n",
    "for corpus in corpus_files:\n",
    "    print(f\"\\nProcessing {corpus}...\")\n",
    "    rexpr = extract_relevant_expressions(corpus, metric='dice')   # CHANGE METRIC HERE -> 'scp' / 'dice' / 'phi_squared' / 'mi'\n",
    "    \n",
    "    # Print top 10 relevant expressions\n",
    "    print(f\"\\nTop 10 Relevant Expressions from {corpus}:\")\n",
    "    for i, (ngram, score, freq) in enumerate(rexpr[:10], 1):\n",
    "        print(f\"{i}. {ngram} (score: {score:.2f}, frequency: {freq})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7b4059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f) Evaluate the results of the extractor through the Precision, Recall and F metric, for at least two corpora. Consider one or more languages.\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217642d2",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99994a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_explicit_keywords(relevant_expr, top_n=15):\n",
    "    return [ngram for ngram, _, _ in relevant_expr[:top_n]]\n",
    "\n",
    "def get_implicit_keywords(doc_text, explicit_keywords, top_n=15):\n",
    "    if not explicit_keywords:\n",
    "        return []\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(vocabulary=explicit_keywords)\n",
    "    try:\n",
    "        doc_vector = vectorizer.fit_transform([doc_text])\n",
    "        keyword_vectors = vectorizer.transform(explicit_keywords)\n",
    "        sims = cosine_similarity(doc_vector, keyword_vectors)[0]\n",
    "        return [explicit_kw for _, explicit_kw in sorted(zip(sims, explicit_keywords), reverse=True)[:top_n]]\n",
    "    except ValueError:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a639a402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus(corpus_path):\n",
    "    results = {}\n",
    "    \n",
    "    for root, _, files in os.walk(corpus_path):\n",
    "        for file in files:\n",
    "            with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                relevant_expr = extract_relevant_expressions(text)\n",
    "                explicit = get_explicit_keywords(relevant_expr)\n",
    "                implicit = get_implicit_keywords(text, explicit)\n",
    "                \n",
    "                results[file] = {\n",
    "                    'explicit': explicit,\n",
    "                    'implicit': implicit\n",
    "                }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3255d986",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "scandir: path too long for Windows",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m process_corpus(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorpus2mw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc, keys \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(results\u001b[38;5;241m.\u001b[39mitems())[:\u001b[38;5;241m3\u001b[39m]:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdoc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExplicit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeys[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplicit\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m3\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mImplicit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeys[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimplicit\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m3\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[24], line 8\u001b[0m, in \u001b[0;36mprocess_corpus\u001b[1;34m(corpus_path)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      7\u001b[0m     text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m----> 8\u001b[0m     relevant_expr \u001b[38;5;241m=\u001b[39m extract_relevant_expressions(text)\n\u001b[0;32m      9\u001b[0m     explicit \u001b[38;5;241m=\u001b[39m get_explicit_keywords(relevant_expr)\n\u001b[0;32m     10\u001b[0m     implicit \u001b[38;5;241m=\u001b[39m get_implicit_keywords(text, explicit)\n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36mextract_relevant_expressions\u001b[1;34m(corpus_path, max_n, min_freq, metric)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_relevant_expressions\u001b[39m(corpus_path, max_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, min_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscp\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Count all ngrams first\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     ngram_counts \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m root, _, files \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mwalk(corpus_path):\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[0;32m      6\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32m<frozen os>:358\u001b[0m, in \u001b[0;36m_walk\u001b[1;34m(top, topdown, onerror, followlinks)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: scandir: path too long for Windows"
     ]
    }
   ],
   "source": [
    "results = process_corpus(\"corpus2mw\")\n",
    "for doc, keys in list(results.items())[:3]:\n",
    "    print(f\"\\n{doc}\\nExplicit: {keys['explicit'][:3]}\\nImplicit: {keys['implicit'][:3]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
