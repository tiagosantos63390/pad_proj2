{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be72bb57",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f70b1356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccae173a",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ca402f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a)\n",
    "def preprocess_text(text):\n",
    "    # Add spaces around special characters\n",
    "    text = re.sub(r'([;,!?<>()\\[\\]&])', r' \\1 ', text)\n",
    "    # Convert to lowercase and split into tokens\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# d)\n",
    "def get_ngrams(tokens, n):\n",
    "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c518bfc4",
   "metadata": {},
   "source": [
    "**Four Cohesion Metrics:**\n",
    "\n",
    "- SCP (Symmetrical Conditional Probability) - measures how likely the components co-occur compared to their independent occurrences\n",
    "\n",
    "- Dice - Measures the overlap between components\n",
    "\n",
    "- φ² (Phi-squared) - Chi-square based association measure\n",
    "\n",
    "- MI (Mutual Information) - Information-theoretic measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02dca6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c)\n",
    "def calculate_scp(n_gram, n_gram_freq, sub_gram_freqs):\n",
    "    numerator = n_gram_freq ** 2\n",
    "    denominator = sum(f1 * f2 for f1, f2 in sub_gram_freqs) / len(sub_gram_freqs)\n",
    "    return numerator / denominator if denominator!=0 else 0\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "def calculate_dice(n_gram, n_gram_freq, sub_gram_freqs):\n",
    "    if len(sub_gram_freqs) == 0:\n",
    "        return 0\n",
    "    \n",
    "    sum_dice = 0\n",
    "    for f1, f2 in sub_gram_freqs:\n",
    "        sum_dice += (2 * n_gram_freq) / (f1 + f2) if (f1 + f2) else 0\n",
    "    \n",
    "    return sum_dice / len(sub_gram_freqs)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "def calculate_phi_squared(n_gram, n_gram_freq, sub_gram_freqs, total_ngrams):\n",
    "    if len(sub_gram_freqs) == 0:\n",
    "        return 0\n",
    "    \n",
    "    sum_phi = 0\n",
    "    for f1, f2 in sub_gram_freqs:\n",
    "        N = total_ngrams\n",
    "        numerator = (N * n_gram_freq - f1 * f2) ** 2\n",
    "        denominator = f1 * f2 * (N - f1) * (N - f2) if (f1 * f2 * (N - f1) * (N - f2)) else 1\n",
    "        sum_phi += numerator / denominator\n",
    "    \n",
    "    return sum_phi / len(sub_gram_freqs)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "def calculate_mi(n_gram, n_gram_freq, sub_gram_freqs, total_ngrams):\n",
    "    if len(sub_gram_freqs) == 0:\n",
    "        return 0\n",
    "    \n",
    "    sum_mi = 0\n",
    "    for f1, f2 in sub_gram_freqs:\n",
    "        if f1 == 0 or f2 == 0:\n",
    "            continue\n",
    "        mi = math.log2((n_gram_freq * total_ngrams) / (f1 * f2))\n",
    "        sum_mi += mi\n",
    "    \n",
    "    return sum_mi / len(sub_gram_freqs) if len(sub_gram_freqs) > 0 else 0\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# choose between metrics\n",
    "def calculate_cohesion_score(ngram, freq, sub_pairs, total_ngrams, metric):\n",
    "    if metric == 'scp':\n",
    "        return calculate_scp(ngram, freq, sub_pairs)\n",
    "    elif metric == 'dice':\n",
    "        return calculate_dice(ngram, freq, sub_pairs)\n",
    "    elif metric == 'phi_squared':\n",
    "        return calculate_phi_squared(ngram, freq, sub_pairs, total_ngrams)\n",
    "    elif metric == 'mi':\n",
    "        return calculate_mi(ngram, freq, sub_pairs, total_ngrams)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid metric. Please choose from 'scp', 'dice', 'phi_squared'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5df447df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relevant_expressions(corpus_path, max_n=7, min_freq=2, metric='scp'):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ngram_counts = defaultdict(int)\n",
    "    nplus1_map = defaultdict(list)\n",
    "    word_counts = defaultdict(int)\n",
    "    total_ngrams = 0\n",
    "\n",
    "    # Count of ngrams (top-down)\n",
    "    for root, _, files in os.walk(corpus_path):\n",
    "        for file in files:\n",
    "            with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                tokens = preprocess_text(f.read())\n",
    "                total_ngrams += len(tokens)\n",
    "                \n",
    "                for i in range(len(tokens)):\n",
    "                    word_counts[tokens[i]] += 1\n",
    "                    \n",
    "                    for n in range(min(max_n, len(tokens)-i), 0, -1):  # descendent\n",
    "                        ngram = ' '.join(tokens[i:i+n])\n",
    "                        ngram_counts[ngram] += 1\n",
    "                        \n",
    "                        if n < max_n and i+n < len(tokens):\n",
    "                            nplus1_map[ngram].append(' '.join(tokens[i:i+n+1]))\n",
    "                            \n",
    "    print(\"All ngrams counted -\", round(time.time() - start_time, 2), \"seconds\")\n",
    "\n",
    "    # Cohesion score\n",
    "    cohesion_scores = {}\n",
    "    for n in range(max_n, 1, -1):\n",
    "        for ngram, freq in ((k,v) for k,v in ngram_counts.items() if len(k.split()) == n):\n",
    "            words = ngram.split()\n",
    "            sub_pairs = [(ngram_counts.get(' '.join(words[:i]), 0), \n",
    "                         ngram_counts.get(' '.join(words[i:]), 0)) \n",
    "                        for i in range(1, len(words))]\n",
    "            \n",
    "            cohesion_scores[ngram] = calculate_cohesion_score(ngram, freq, sub_pairs, total_ngrams, metric)\n",
    "\n",
    "    print(\"All cohesion scores calculated -\", round(time.time() - start_time, 2), \"seconds\")\n",
    "\n",
    "    # LocalMaxs filter\n",
    "    results = []\n",
    "    for ngram, score in cohesion_scores.items():\n",
    "        words = ngram.split()\n",
    "        freq = ngram_counts[ngram]\n",
    "        \n",
    "        # Skip if below frequency min or single word\n",
    "        if freq < min_freq or len(words) < 2:\n",
    "            continue\n",
    "            \n",
    "        # Calculate max sub-score (n-1 grams)\n",
    "        sub_scores = []\n",
    "        for i in range(len(words)):\n",
    "            sub_ngram = ' '.join(words[:i] + words[i+1:])\n",
    "            sub_score = cohesion_scores.get(sub_ngram, 0)\n",
    "            sub_scores.append(sub_score)\n",
    "        max_sub = max(sub_scores) if sub_scores else 0\n",
    "        \n",
    "        # Calculate max parent score (n+1 grams)\n",
    "        parent_scores = [cohesion_scores.get(p, 0) for p in nplus1_map.get(ngram, [])]\n",
    "        max_parent = max(parent_scores) if parent_scores else 0\n",
    "        \n",
    "        threshold = math.sqrt(0.5 * (max_sub**2 + max_parent**2))\n",
    "        if score > threshold:\n",
    "            results.append((ngram, score, freq))\n",
    "            \n",
    "    print(f\"Finished '{corpus_path}' - {round(time.time() - start_time, 2)} seconds\")\n",
    "\n",
    "    return sorted(results, key=lambda x: (-len(x[0].split()), -x[1], -x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18a63c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 'corpus2mw'...\n",
      "All ngrams counted - 43.19 seconds\n",
      "All cohesion scores calculated - 124.36 seconds\n",
      "Finished 'corpus2mw' - 133.84 seconds\n",
      "\n",
      "Top 10 Relevant Expressions from corpus2mw:\n",
      "1. newsbank com nl search we archives p_product (score: 1.00, frequency: 2)\n",
      "2. p_action search p_maxdocs 200 p_topdoc 1 p_text_direct (score: 1.00, frequency: 2)\n",
      "3. p_field_direct 0 document_id p_perpage 10 p_sort ymd_date (score: 1.00, frequency: 2)\n",
      "4. document_id p_perpage 10 p_sort ymd_date d s_trackval (score: 1.00, frequency: 2)\n",
      "5. p_perpage 10 p_sort ymd_date d s_trackval googlepm (score: 1.00, frequency: 2)\n",
      "6. prokaryotic names with standing in nomenclature lpsn (score: 1.00, frequency: 2)\n",
      "7. url http en wikipedia org wiki curid (score: 1.00, frequency: 4788)\n",
      "8. starlings are small to medium sized passerine (score: 0.95, frequency: 3)\n",
      "9. nemili is a panchayat town in vellore (score: 0.67, frequency: 2)\n",
      "10. synaptic potential that makes a postsynaptic neuron (score: 0.60, frequency: 2)\n"
     ]
    }
   ],
   "source": [
    "# Test with first smaller corpus    'corpus2mw'\n",
    "\n",
    "print(\"\\nProcessing 'corpus2mw'...\")\n",
    "rexpr = extract_relevant_expressions('corpus2mw')   # SCP Metric\n",
    "\n",
    "# Print top 10 relevant expressions\n",
    "print(f\"\\nTop 10 Relevant Expressions from corpus2mw:\")\n",
    "for i, (ngram, score, freq) in enumerate(rexpr[:10], 1):\n",
    "    print(f\"{i}. {ngram} (score: {score:.2f}, frequency: {freq})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436af9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing corpus2mw...\n",
      "All ngrams counted - 156.18\n",
      "All cohesion scores calculated - 247.52\n",
      "\n",
      "Top 10 Relevant Expressions from corpus2mw:\n",
      "1. newsbank com nl search we archives p_product (score: 1.00, frequency: 2)\n",
      "2. p_action search p_maxdocs 200 p_topdoc 1 p_text_direct (score: 1.00, frequency: 2)\n",
      "3. p_field_direct 0 document_id p_perpage 10 p_sort ymd_date (score: 1.00, frequency: 2)\n",
      "4. document_id p_perpage 10 p_sort ymd_date d s_trackval (score: 1.00, frequency: 2)\n",
      "5. p_perpage 10 p_sort ymd_date d s_trackval googlepm (score: 1.00, frequency: 2)\n",
      "6. prokaryotic names with standing in nomenclature lpsn (score: 1.00, frequency: 2)\n",
      "7. url http en wikipedia org wiki curid (score: 1.00, frequency: 4788)\n",
      "8. starlings are small to medium sized passerine (score: 0.95, frequency: 3)\n",
      "9. nemili is a panchayat town in vellore (score: 0.67, frequency: 2)\n",
      "10. synaptic potential that makes a postsynaptic neuron (score: 0.60, frequency: 2)\n",
      "\n",
      "Processing corpus4mw...\n"
     ]
    }
   ],
   "source": [
    "corpus_files = ['corpus2mw', 'corpus4mw']\n",
    "    \n",
    "# Process both corpora files\n",
    "for corpus in corpus_files:\n",
    "    print(f\"\\nProcessing {corpus}...\")\n",
    "    rexpr = extract_relevant_expressions(corpus)   # SCP Metric\n",
    "    \n",
    "    # Print top 10 relevant expressions\n",
    "    print(f\"\\nTop 10 Relevant Expressions from {corpus}:\")\n",
    "    for i, (ngram, score, freq) in enumerate(rexpr[:10], 1):\n",
    "        print(f\"{i}. {ngram} (score: {score:.2f}, frequency: {freq})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ae4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now with other cohesion scores\n",
    "\n",
    "metric = 'dice'   # CHANGE METRIC HERE -> 'scp' / 'dice' / 'phi_squared' / 'mi'\n",
    "\n",
    "for corpus in corpus_files:\n",
    "    print(f\"\\nProcessing {corpus}...\")\n",
    "    rexpr = extract_relevant_expressions(corpus, metric=metric)\n",
    "    \n",
    "    # Print top 5 relevant expressions\n",
    "    print(f\"\\nTop 5 Relevant Expressions from {corpus} (with metric '{metric}'):\")\n",
    "    for i, (ngram, score, freq) in enumerate(rexpr[:5], 1):\n",
    "        print(f\"{i}. {ngram} (score: {score:.2f}, frequency: {freq})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b4059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f) Evaluate the results of the extractor through the Precision, Recall and F metric, for at least two corpora. Consider one or more languages.\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217642d2",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99994a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_explicit_keywords(relevant_expr, top_n=15):\n",
    "    return [ngram for ngram, _, _ in relevant_expr[:top_n]]\n",
    "\n",
    "def get_implicit_keywords(doc_text, explicit_keywords, top_n=15):\n",
    "    if not explicit_keywords:\n",
    "        return []\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(vocabulary=explicit_keywords)\n",
    "    try:\n",
    "        doc_vector = vectorizer.fit_transform([doc_text])\n",
    "        keyword_vectors = vectorizer.transform(explicit_keywords)\n",
    "        sims = cosine_similarity(doc_vector, keyword_vectors)[0]\n",
    "        return [explicit_kw for _, explicit_kw in sorted(zip(sims, explicit_keywords), reverse=True)[:top_n]]\n",
    "    except ValueError:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a639a402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_corpus(corpus_path):\n",
    "    results = {}\n",
    "    \n",
    "    for root, _, files in os.walk(corpus_path):\n",
    "        for file in files:\n",
    "            with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                relevant_expr = extract_relevant_expressions(text)\n",
    "                explicit = get_explicit_keywords(relevant_expr)\n",
    "                implicit = get_implicit_keywords(text, explicit)\n",
    "                \n",
    "                results[file] = {\n",
    "                    'explicit': explicit,\n",
    "                    'implicit': implicit\n",
    "                }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3255d986",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = process_corpus(\"corpus2mw\")\n",
    "for doc, keys in list(results.items())[:3]:\n",
    "    print(f\"\\n{doc}\\nExplicit: {keys['explicit'][:3]}\\nImplicit: {keys['implicit'][:3]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
