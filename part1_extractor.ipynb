{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be72bb57",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70b1356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccae173a",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca402f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Add spaces around special characters\n",
    "    text = re.sub(r'([;,!?<>()\\[\\]&])', r' \\1 ', text)\n",
    "    # Convert to lowercase and split into tokens\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "def get_ngrams(tokens, n):\n",
    "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c518bfc4",
   "metadata": {},
   "source": [
    "**Four Cohesion Metrics:**\n",
    "\n",
    "- SCP (Symmetrical Conditional Probability) - measures how likely the components co-occur compared to their independent occurrences\n",
    "\n",
    "- Dice - Measures the overlap between components\n",
    "\n",
    "- φ² (Phi-squared) - Chi-square based association measure\n",
    "\n",
    "- MI (Mutual Information) - Information-theoretic measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dca6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_scp(n_gram, n_gram_freq, sub_gram_freqs):\n",
    "    numerator = n_gram_freq ** 2\n",
    "    denominator = sum(f1 * f2 for f1, f2 in sub_gram_freqs) / len(sub_gram_freqs)\n",
    "    return numerator / denominator if denominator else 0\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "def calculate_dice(n_gram, n_gram_freq, sub_gram_freqs):\n",
    "    if len(sub_gram_freqs) == 0:\n",
    "        return 0\n",
    "    \n",
    "    sum_dice = 0\n",
    "    for f1, f2 in sub_gram_freqs:\n",
    "        sum_dice += (2 * n_gram_freq) / (f1 + f2) if (f1 + f2) else 0\n",
    "    \n",
    "    return sum_dice / len(sub_gram_freqs)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "def calculate_phi_squared(n_gram, n_gram_freq, sub_gram_freqs, total_ngrams):\n",
    "    if len(sub_gram_freqs) == 0:\n",
    "        return 0\n",
    "    \n",
    "    sum_phi = 0\n",
    "    for f1, f2 in sub_gram_freqs:\n",
    "        N = total_ngrams\n",
    "        numerator = (N * n_gram_freq - f1 * f2) ** 2\n",
    "        denominator = f1 * f2 * (N - f1) * (N - f2) if (f1 * f2 * (N - f1) * (N - f2)) else 1\n",
    "        sum_phi += numerator / denominator\n",
    "    \n",
    "    return sum_phi / len(sub_gram_freqs)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "def calculate_mi(n_gram, n_gram_freq, sub_gram_freqs, total_ngrams):\n",
    "    if len(sub_gram_freqs) == 0:\n",
    "        return 0\n",
    "    \n",
    "    sum_mi = 0\n",
    "    for f1, f2 in sub_gram_freqs:\n",
    "        if f1 == 0 or f2 == 0:\n",
    "            continue\n",
    "        mi = math.log2((n_gram_freq * total_ngrams) / (f1 * f2))\n",
    "        sum_mi += mi\n",
    "    \n",
    "    return sum_mi / len(sub_gram_freqs) if len(sub_gram_freqs) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe55d297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relevant_expressions(corpus_path, max_n=7, min_freq=2):\n",
    "    # Count n-grams and their frequencies\n",
    "    ngram_counts = defaultdict(int)\n",
    "    for root, _, files in os.walk(corpus_path):\n",
    "        for file in files:\n",
    "            with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                tokens = preprocess_text(text)\n",
    "                for n in range(2, max_n+1):\n",
    "                    for ngram in get_ngrams(tokens, n):\n",
    "                        ngram_counts[ngram] += 1\n",
    "\n",
    "    # Filter by minimum frequency\n",
    "    ngram_counts = {k: v for k, v in ngram_counts.items() if v >= min_freq}\n",
    "\n",
    "    # Calculate cohesion scores\n",
    "    cohesion_scores = {}\n",
    "    for ngram, freq in ngram_counts.items():\n",
    "        words = ngram.split()\n",
    "        n = len(words)\n",
    "        \n",
    "        if n == 2:\n",
    "            f_xy = freq\n",
    "            f_x = ngram_counts.get(words[0], 0)\n",
    "            f_y = ngram_counts.get(words[1], 0)\n",
    "            cohesion_scores[ngram] = (f_xy ** 2) / (f_x * f_y) if (f_x * f_y) else 0\n",
    "        else:\n",
    "            sub_pairs = []\n",
    "            for i in range(1, n):\n",
    "                left = ' '.join(words[:i])\n",
    "                right = ' '.join(words[i:])\n",
    "                f_left = ngram_counts.get(left, 0)\n",
    "                f_right = ngram_counts.get(right, 0)\n",
    "                sub_pairs.append((f_left, f_right))\n",
    "            \n",
    "            cohesion_scores[ngram] = calculate_scp(ngram, freq, sub_pairs)   # change here the cohesion\tmetric \n",
    "\n",
    "    # LocalMaxs algorithm\n",
    "    relevant_expr = []\n",
    "    for ngram in cohesion_scores:\n",
    "        words = ngram.split()\n",
    "        n = len(words)\n",
    "        current_score = cohesion_scores[ngram]\n",
    "        \n",
    "        # Check if current ngram is a local maximum\n",
    "        is_local_max = True\n",
    "        \n",
    "        # Check (n-1)-grams\n",
    "        if n > 2:\n",
    "            for i in range(n-1):\n",
    "                sub_ngram = ' '.join(words[:i] + words[i+1:])\n",
    "                if cohesion_scores.get(sub_ngram, 0) >= current_score:\n",
    "                    is_local_max = False\n",
    "                    break\n",
    "        \n",
    "        # Check (n+1)-grams (simplified - we dont have all possible (n+1)-grams)\n",
    "        if is_local_max and n < max_n:\n",
    "            # In a complete implementation, we check all possible (n+1)-grams containing this ngram\n",
    "            pass\n",
    "        \n",
    "        if is_local_max:\n",
    "            relevant_expr.append((ngram, cohesion_scores[ngram], ngram_counts[ngram]))\n",
    "\n",
    "    # Sort by cohesion score descending\n",
    "    relevant_expr.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return relevant_expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "436af9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing corpus2mw...\n",
      "\n",
      "Top 20 Relevant Expressions from corpus2mw:\n",
      "1. credited her second grade (score: 3.00, freq: 2)\n",
      "2. males had a median (score: 3.00, freq: 37)\n",
      "3. the bonneville salt flats (score: 3.00, freq: 2)\n",
      "4. kennet and avon canal (score: 3.00, freq: 3)\n",
      "5. someone living alone who (score: 3.00, freq: 49)\n",
      "6. beta theta pi fraternity (score: 3.00, freq: 2)\n",
      "7. http nl newsbank com (score: 3.00, freq: 2)\n",
      "8. nl newsbank com nl (score: 3.00, freq: 2)\n",
      "9. newsbank com nl search (score: 3.00, freq: 2)\n",
      "10. com nl search we (score: 3.00, freq: 2)\n",
      "11. nl search we archives (score: 3.00, freq: 2)\n",
      "12. search we archives p_product (score: 3.00, freq: 2)\n",
      "13. p_action search p_maxdocs 200 (score: 3.00, freq: 2)\n",
      "14. search p_maxdocs 200 p_topdoc (score: 3.00, freq: 2)\n",
      "15. p_maxdocs 200 p_topdoc 1 (score: 3.00, freq: 2)\n",
      "16. 200 p_topdoc 1 p_text_direct (score: 3.00, freq: 2)\n",
      "17. p_topdoc 1 p_text_direct 0 (score: 3.00, freq: 2)\n",
      "18. p_field_direct 0 document_id p_perpage (score: 3.00, freq: 2)\n",
      "19. 0 document_id p_perpage 10 (score: 3.00, freq: 2)\n",
      "20. document_id p_perpage 10 p_sort (score: 3.00, freq: 2)\n",
      "\n",
      "Processing corpus4mw...\n",
      "\n",
      "Top 20 Relevant Expressions from corpus4mw:\n",
      "1. http en wikipedia org (score: 3.00, freq: 9457)\n",
      "2. en wikipedia org wiki (score: 3.00, freq: 9457)\n",
      "3. id q2u0wwtfvwgc pg pa28 (score: 3.00, freq: 2)\n",
      "4. q2u0wwtfvwgc pg pa28 dq (score: 3.00, freq: 2)\n",
      "5. pg pa28 dq laura (score: 3.00, freq: 2)\n",
      "6. pa28 dq laura mulvey (score: 3.00, freq: 2)\n",
      "7. dq laura mulvey visual (score: 3.00, freq: 2)\n",
      "8. narrative cinema as_brr 3 (score: 3.00, freq: 2)\n",
      "9. cinema as_brr 3 ei (score: 3.00, freq: 2)\n",
      "10. as_brr 3 ei uf3rsfpjd4pcygsqz_zzbg (score: 3.00, freq: 2)\n",
      "11. fifa can emerge stronger (score: 3.00, freq: 2)\n",
      "12. someone living alone who (score: 3.00, freq: 94)\n",
      "13. of yulia tymoshenko and (score: 3.00, freq: 2)\n",
      "14. the entorhinal cortex ec (score: 3.00, freq: 2)\n",
      "15. rté raidió na gaeltachta (score: 3.00, freq: 2)\n",
      "16. rapsodie de la main (score: 3.00, freq: 2)\n",
      "17. proyecto la venta ii (score: 3.00, freq: 2)\n",
      "18. the hanna barbera studio (score: 3.00, freq: 2)\n",
      "19. annual portfolio budget statements (score: 3.00, freq: 2)\n",
      "20. pre 20th century boardgame (score: 3.00, freq: 2)\n"
     ]
    }
   ],
   "source": [
    "corpus_files = ['corpus2mw', 'corpus4mw']\n",
    "    \n",
    "# Process both corpora files\n",
    "for corpus in corpus_files:\n",
    "    print(f\"\\nProcessing {corpus}...\")\n",
    "    rexpr = extract_relevant_expressions(corpus)\n",
    "    \n",
    "    # Print top 20 relevant expressions\n",
    "    print(f\"\\nTop 20 Relevant Expressions from {corpus}:\")\n",
    "    for i, (ngram, score, freq) in enumerate(rexpr[:20], 1):\n",
    "        print(f\"{i}. {ngram} (score: {score:.2f}, freq: {freq})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ae4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217642d2",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45ef063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
